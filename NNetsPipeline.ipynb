{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced2a570-5bbf-4031-9034-4b433794ce37",
   "metadata": {},
   "source": [
    "## Building the whole pipeline in bits !! ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c4e3d-0056-4090-bba1-9463b68a4d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e550661-2227-4b40-94d4-30efde74b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nnfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740976f5-a9a4-43a9-a62e-1d70435e8e22",
   "metadata": {},
   "source": [
    "### Layer Construction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e19dc7-ef06-4c36-960c-2354d62e8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self,inputs,neurons):\n",
    "        self.weights = 0.01* np.random.randn(inputs,neurons)\n",
    "        self.bias = np.zeros((1,neurons))\n",
    "    def forward_prop(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs,self.weights)+ self.bias\n",
    "\n",
    "    def backward_prop(self,dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases = np.sum(dvalues,axis=0,keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb3381-32d2-4b53-abbf-eaee88136117",
   "metadata": {},
   "source": [
    "### Activation Function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cdee11-e51d-4881-b2d4-b867744a17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class activateReLu:\n",
    "    def forward_pass(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "    def backward_pass(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2430830c-6df0-4ba9-b2e9-2dd576599416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class activateSoftmax:\n",
    "    def forward_pass(self,inputs):\n",
    "        expVals = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        probs = expVals / np.sum(expVals,axis=1,keepdims=True)\n",
    "        self.output = probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec710c49-012a-4c84-9e99-3f3be32d809b",
   "metadata": {},
   "source": [
    "### Loss Function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ff3ce-f9bd-45bb-97aa-1b74517f1279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def lossCal(self,output,y):\n",
    "        sample_losses = self.forward_pass(output,y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dfda89-bd25-4995-8b8b-dcc269bafb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyFunc(Loss) :\n",
    "    def forward_pass(self,y_pred,y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clip = np.clip(y_pred,1e-7,1-1e-7)\n",
    "\n",
    "        if len(y_true.shape) ==1:\n",
    "            correct_confidence = y_pred_clip[range(samples),y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidence = np.sum(y_true*y_pred_clip,axis=1)\n",
    "\n",
    "        neg_likelihoods = -np.log(correct_confidence)\n",
    "        return neg_likelihoods\n",
    "\n",
    "\n",
    "    def backward_prop(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true/dvalues\n",
    "        self.dinputs = self.dinputs/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0846e7-b74a-491f-8efd-cf264ce4598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_Categorical_Loss():\n",
    "    def __init__(self):\n",
    "        self.activation = activateSoftmax()\n",
    "        self.loss = CategoricalCrossEntropyFunc()\n",
    "\n",
    "    def forward_prop(self, inputs, y_true):\n",
    "        self.activation.forward_pass(inputs)\n",
    "        self.output = self.activation.output\n",
    "\n",
    "        return self.loss.lossCal(self.output,y_true)\n",
    "\n",
    "    def backward_prop(self,dvalues,y_true):\n",
    "        samples = len(dvalues)\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53274913-6a81-47ea-9b55-74af7353d452",
   "metadata": {},
   "source": [
    "### Final Testing on Data !!! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710d775-b4c5-477a-a1b5-ee5bf30ebd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "nnfs.init()\n",
    "\n",
    "X,y = spiral_data(samples=100, classes=3)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a26f6-f58a-46b6-bd80-6ebd3979b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X,y = spiral_data(samples=100, classes=3)\n",
    "layer1 = Dense(2,3)\n",
    "activation_layer_1 = activateReLu()\n",
    "\n",
    "layer2 = Dense(3,3)\n",
    "loss_activation = Softmax_Categorical_Loss()\n",
    "\n",
    "layer1.forward_prop(X)\n",
    "activation_layer_1.forward_pass(layer1.output)\n",
    "\n",
    "layer2.forward_prop(activation_layer_1.output)\n",
    "\n",
    "loss = loss_activation.forward_prop(layer2.output,y)\n",
    "\n",
    "print(\"### Results before forward propagation ###\")\n",
    "print(loss_activation.output[:5])\n",
    "\n",
    "print(\"Loss : \", loss)\n",
    "\n",
    "preds = np.argmax(loss_activation.output, axis=1)\n",
    "# print(preds)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y,axis=1)\n",
    "# print(y)\n",
    "\n",
    "acc = np.mean(preds == y)\n",
    "print(\"Accuracy : \", acc)\n",
    "\n",
    "loss_back = loss_activation.backward_prop(loss_activation.output,y)\n",
    "layer2.backward_prop(loss_activation.dinputs)\n",
    "activation_layer_1.backward_pass(layer2.dinputs)\n",
    "layer1.backward_prop(activation_layer_1.dinputs)\n",
    "print(\"### Results before backward propagation ###\")\n",
    "# Print gradients\n",
    "print(layer1.dweights)\n",
    "print(layer1.dbiases)\n",
    "print(layer2.dweights)\n",
    "print(layer2.dbiases)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74bbe3-5ddb-4597-a0de-a835f81e92ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7c718-4b43-4e87-8c1e-b626ca2d986b",
   "metadata": {},
   "source": [
    "### Building the Optimizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07f634-f128-4c48-af2d-f5cc54b8b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD:\n",
    "    def __init__(self,learning_rate=0.09):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        layer.weights += -self.learning_rate*layer.dweights\n",
    "        layer.bias += -self.learning_rate*layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb1813-9870-4da2-b07c-7dd2d2db9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(samples=100, classes=3)\n",
    "layer1 = Dense(2,64)\n",
    "activation_layer_1 = activateReLu()\n",
    "\n",
    "layer2 = Dense(64,3)\n",
    "loss_activation = Softmax_Categorical_Loss()\n",
    "optim = Optimizer_GD()\n",
    "\n",
    "epochs = []\n",
    "accuracies = []\n",
    "for epoch in range (10001):\n",
    "    layer1.forward_prop(X)\n",
    "    activation_layer_1.forward_pass(layer1.output)\n",
    "    \n",
    "    layer2.forward_prop(activation_layer_1.output)\n",
    "    \n",
    "    loss = loss_activation.forward_prop(layer2.output,y)\n",
    "    \n",
    "    # print(\"### Results before forward propagation ###\")\n",
    "    # print(loss_activation.output[:5])\n",
    "    \n",
    "    \n",
    "    \n",
    "    preds = np.argmax(loss_activation.output, axis=1)\n",
    "    # print(preds)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y,axis=1)\n",
    "    # print(y)\n",
    "    \n",
    "    acc = np.mean(preds == y)\n",
    "    # print(\"Accuracy : \", acc)\n",
    "    if not epoch %100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {acc:.3f}, ' +\n",
    "              f'loss: {loss:.3f}')\n",
    "        epochs.append(epoch)\n",
    "        accuracies.append(acc)\n",
    "    loss_activation.backward_prop(loss_activation.output,y)\n",
    "    layer2.backward_prop(loss_activation.dinputs)\n",
    "    activation_layer_1.backward_pass(layer2.dinputs)\n",
    "    layer1.backward_prop(activation_layer_1.dinputs)\n",
    "\n",
    "    optim.update_params(layer1)\n",
    "    optim.update_params(layer2)\n",
    "# print(\"### Results before backward propagation ###\")\n",
    "# Print gradients\n",
    "# print(layer1.dweights)\n",
    "# print(layer1.dbiases)\n",
    "# print(layer2.dweights)\n",
    "# print(layer2.dbiases)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(epochs, accuracies, color='blue', label='Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Epochs vs Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba68ae-e923-428e-9e7b-571e38829e47",
   "metadata": {},
   "source": [
    "### Learning Rate Decay ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeba2d4-a15f-488d-8a1e-ae03127160b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD:\n",
    "    def __init__(self,learning_rate=0.0, decay= 0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.curr_learning_rate = learning_rate\n",
    "        self.iterations = 0 \n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.curr_learning_rate = self.learning_rate * (1.0/(1+self.decay*self.iterations))\n",
    "    def update_params(self,layer):\n",
    "        layer.weights += -self.curr_learning_rate*layer.dweights\n",
    "        layer.bias += -self.curr_learning_rate*layer.dbiases\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0965cb20-a8a7-40a3-8379-5a039b1312e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(samples=100, classes=3)\n",
    "layer1 = Dense(2,64)\n",
    "activation_layer_1 = activateReLu()\n",
    "\n",
    "layer2 = Dense(64,3)\n",
    "loss_activation = Softmax_Categorical_Loss()\n",
    "optim = Optimizer_GD(learning_rate=2.0,decay=1e-3)\n",
    "\n",
    "lrs = []\n",
    "accuracies = []\n",
    "for epoch in range (20001):\n",
    "    layer1.forward_prop(X)\n",
    "    activation_layer_1.forward_pass(layer1.output)\n",
    "    \n",
    "    layer2.forward_prop(activation_layer_1.output)\n",
    "    \n",
    "    loss = loss_activation.forward_prop(layer2.output,y)\n",
    "    \n",
    "    # print(\"### Results before forward propagation ###\")\n",
    "    # print(loss_activation.output[:5])\n",
    "    \n",
    "    \n",
    "    \n",
    "    preds = np.argmax(loss_activation.output, axis=1)\n",
    "    # print(preds)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y,axis=1)\n",
    "    # print(y)\n",
    "    \n",
    "    acc = np.mean(preds == y)\n",
    "    # print(\"Accuracy : \", acc)\n",
    "    if not epoch %100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {acc:.3f}, ' +\n",
    "              f'loss: {loss:.3f},' +\n",
    "             f'lr: {optim.curr_learning_rate}')\n",
    "        lrs.append(optim.curr_learning_rate)\n",
    "        accuracies.append(acc)\n",
    "    loss_activation.backward_prop(loss_activation.output,y)\n",
    "    layer2.backward_prop(loss_activation.dinputs)\n",
    "    activation_layer_1.backward_pass(layer2.dinputs)\n",
    "    layer1.backward_prop(activation_layer_1.dinputs)\n",
    "\n",
    "    optim.pre_update_params()\n",
    "    optim.update_params(layer1)\n",
    "    optim.update_params(layer2)\n",
    "    optim.post_update_params()\n",
    "# print(\"### Results before backward propagation ###\")\n",
    "# Print gradients\n",
    "# print(layer1.dweights)\n",
    "# print(layer1.dbiases)\n",
    "# print(layer2.dweights)\n",
    "# print(layer2.dbiases)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(lrs, accuracies, color='blue', label='Accuracy')\n",
    "plt.xlabel('LRs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('LRs vs Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768f41e-fd8b-4bc7-a1d1-a4948d5e1413",
   "metadata": {},
   "source": [
    "### Momentum ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4433018-6604-4be5-aa5b-4a460eab4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD:\n",
    "    def __init__(self,learning_rate=0.0, decay= 0.0, momen=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.curr_learning_rate = learning_rate\n",
    "        self.iterations = 0 \n",
    "        self.momentum = momen\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.curr_learning_rate = self.learning_rate * (1.0/(1+self.decay*self.iterations))\n",
    "    def update_params(self,layer):\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer,'weights_momentums'):\n",
    "                layer.weights_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.bias)\n",
    "\n",
    "            weights_updates = self.momentum * layer.weights_momentums - self.curr_learning_rate*layer.dweights\n",
    "            layer.weights_momentums = weights_updates\n",
    "\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.curr_learning_rate*layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        else:\n",
    "            layer.weights += -self.curr_learning_rate*layer.dweights\n",
    "            layer.bias += -self.curr_learning_rate*layer.dbiases\n",
    "\n",
    "        layer.weights += weights_updates\n",
    "        layer.bias += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e70c9-f82c-45ca-966c-41aedca6615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(samples=100, classes=3)\n",
    "layer1 = Dense(2,64)\n",
    "activation_layer_1 = activateReLu()\n",
    "\n",
    "layer2 = Dense(64,3)\n",
    "loss_activation = Softmax_Categorical_Loss()\n",
    "optim = Optimizer_GD(learning_rate=2.0,decay=1e-3,momen=0.09)\n",
    "\n",
    "lrs = []\n",
    "accuracies = []\n",
    "for epoch in range (10001):\n",
    "    layer1.forward_prop(X)\n",
    "    activation_layer_1.forward_pass(layer1.output)\n",
    "    \n",
    "    layer2.forward_prop(activation_layer_1.output)\n",
    "    \n",
    "    loss = loss_activation.forward_prop(layer2.output,y)\n",
    "    \n",
    "    # print(\"### Results before forward propagation ###\")\n",
    "    # print(loss_activation.output[:5])\n",
    "    \n",
    "    \n",
    "    \n",
    "    preds = np.argmax(loss_activation.output, axis=1)\n",
    "    # print(preds)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y,axis=1)\n",
    "    # print(y)\n",
    "    \n",
    "    acc = np.mean(preds == y)\n",
    "    # print(\"Accuracy : \", acc)\n",
    "    if not epoch %100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {acc:.3f}, ' +\n",
    "              f'loss: {loss:.3f},' +\n",
    "             f'lr: {optim.curr_learning_rate}')\n",
    "        lrs.append(optim.curr_learning_rate)\n",
    "        accuracies.append(acc)\n",
    "    loss_activation.backward_prop(loss_activation.output,y)\n",
    "    layer2.backward_prop(loss_activation.dinputs)\n",
    "    activation_layer_1.backward_pass(layer2.dinputs)\n",
    "    layer1.backward_prop(activation_layer_1.dinputs)\n",
    "\n",
    "    optim.pre_update_params()\n",
    "    optim.update_params(layer1)\n",
    "    optim.update_params(layer2)\n",
    "    optim.post_update_params()\n",
    "# print(\"### Results before backward propagation ###\")\n",
    "# Print gradients\n",
    "# print(layer1.dweights)\n",
    "# print(layer1.dbiases)\n",
    "# print(layer2.dweights)\n",
    "# print(layer2.dbiases)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(lrs, accuracies, color='blue', label='Accuracy')\n",
    "plt.xlabel('LRs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('LRs vs Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0732ebd3-828f-41bb-a7d1-6c3eb3661bbf",
   "metadata": {},
   "source": [
    "### Adagrad Optimizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e842280-fbb3-492d-82b8-f8c55f8cd54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adagrad:\n",
    "    def __init__(self,learning_rate=0.0, decay=0., epsilion=0.09):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.curr_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilion = epsilion\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.curr_learning_rate = self.learning_rate * (1.0/1.0+(self.decay*self.iterations))\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer,'weights_cache'):\n",
    "            layer.weights_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.bias)\n",
    "\n",
    "        layer.weights_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        layer.weights += -self.curr_learning_rate*layer.dweights/(np.sqrt(layer.weights_cache+self.epsilion))\n",
    "        layer.bias += -self.curr_learning_rate*layer.dbiases/(np.sqrt(layer.bias_cache+self.epsilion))\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea9555-4c7f-430f-87af-80536eb642be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(samples=100, classes=3)\n",
    "layer1 = Dense(2,64)\n",
    "activation_layer_1 = activateReLu()\n",
    "\n",
    "layer2 = Dense(64,3)\n",
    "loss_activation = Softmax_Categorical_Loss()\n",
    "optim = Optimizer_Adagrad(learning_rate=1.0,decay=1e-3)\n",
    "\n",
    "lrs = []\n",
    "accuracies = []\n",
    "for epoch in range (10001):\n",
    "    layer1.forward_prop(X)\n",
    "    activation_layer_1.forward_pass(layer1.output)\n",
    "    \n",
    "    layer2.forward_prop(activation_layer_1.output)\n",
    "    \n",
    "    loss = loss_activation.forward_prop(layer2.output,y)\n",
    "    \n",
    "    # print(\"### Results before forward propagation ###\")\n",
    "    # print(loss_activation.output[:5])\n",
    "    \n",
    "    \n",
    "    \n",
    "    preds = np.argmax(loss_activation.output, axis=1)\n",
    "    # print(preds)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y,axis=1)\n",
    "    # print(y)\n",
    "    \n",
    "    acc = np.mean(preds == y)\n",
    "    # print(\"Accuracy : \", acc)\n",
    "    if not epoch %100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {acc:.3f}, ' +\n",
    "              f'loss: {loss:.3f},' +\n",
    "             f'lr: {optim.curr_learning_rate}')\n",
    "        lrs.append(optim.curr_learning_rate)\n",
    "        accuracies.append(acc)\n",
    "    loss_activation.backward_prop(loss_activation.output,y)\n",
    "    layer2.backward_prop(loss_activation.dinputs)\n",
    "    activation_layer_1.backward_pass(layer2.dinputs)\n",
    "    layer1.backward_prop(activation_layer_1.dinputs)\n",
    "\n",
    "    optim.pre_update_params()\n",
    "    optim.update_params(layer1)\n",
    "    optim.update_params(layer2)\n",
    "    optim.post_update_params()\n",
    "# print(\"### Results before backward propagation ###\")\n",
    "# Print gradients\n",
    "# print(layer1.dweights)\n",
    "# print(layer1.dbiases)\n",
    "# print(layer2.dweights)\n",
    "# print(layer2.dbiases)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(lrs, accuracies, color='blue', label='Accuracy')\n",
    "plt.xlabel('LRs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('LRs vs Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aec75a-cda7-4a40-b107-3369e7efb4c1",
   "metadata": {},
   "source": [
    "### RMSProp Optimizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1243e-6cc9-42f3-991b-5cafdf281701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSProp:\n",
    "    def __init__(self,learning_rate=0.0, decay=0., epsilion=0.09, rho= 0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.curr_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.rho = rho\n",
    "        self.epsilion = epsilion\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.curr_learning_rate = self.learning_rate * (1.0/1.0+(self.decay*self.iterations))\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer,'weights_cache'):\n",
    "            layer.weights_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.bias)\n",
    "\n",
    "        layer.weights_cache = self.rho*layer.weights_cache + (1-self.rho)*layer.dweights**2\n",
    "        layer.bias_cache = self.rho*layer.bias_cache + (1-self.rho)*layer.dbiases**2\n",
    "\n",
    "        layer.weights += -self.curr_learning_rate*layer.dweights/(np.sqrt(layer.weights_cache+self.epsilion))\n",
    "        layer.bias += -self.curr_learning_rate*layer.dbiases/(np.sqrt(layer.bias_cache+self.epsilion))\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42f847-b8fd-42fb-ba63-8c67ad7db586",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(samples=100, classes=3)\n",
    "layer1 = Dense(2,64)\n",
    "activation_layer_1 = activateReLu()\n",
    "\n",
    "layer2 = Dense(64,3)\n",
    "loss_activation = Softmax_Categorical_Loss()\n",
    "optim = Optimizer_RMSProp(learning_rate=1.0,decay=1e-3)\n",
    "\n",
    "lrs = []\n",
    "accuracies = []\n",
    "for epoch in range (10001):\n",
    "    layer1.forward_prop(X)\n",
    "    activation_layer_1.forward_pass(layer1.output)\n",
    "    \n",
    "    layer2.forward_prop(activation_layer_1.output)\n",
    "    \n",
    "    loss = loss_activation.forward_prop(layer2.output,y)\n",
    "    \n",
    "    # print(\"### Results before forward propagation ###\")\n",
    "    # print(loss_activation.output[:5])\n",
    "    \n",
    "    \n",
    "    \n",
    "    preds = np.argmax(loss_activation.output, axis=1)\n",
    "    # print(preds)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y,axis=1)\n",
    "    # print(y)\n",
    "    \n",
    "    acc = np.mean(preds == y)\n",
    "    # print(\"Accuracy : \", acc)\n",
    "    if not epoch %100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {acc:.3f}, ' +\n",
    "              f'loss: {loss:.3f},' +\n",
    "             f'lr: {optim.curr_learning_rate}')\n",
    "        lrs.append(optim.curr_learning_rate)\n",
    "        accuracies.append(acc)\n",
    "    loss_activation.backward_prop(loss_activation.output,y)\n",
    "    layer2.backward_prop(loss_activation.dinputs)\n",
    "    activation_layer_1.backward_pass(layer2.dinputs)\n",
    "    layer1.backward_prop(activation_layer_1.dinputs)\n",
    "\n",
    "    optim.pre_update_params()\n",
    "    optim.update_params(layer1)\n",
    "    optim.update_params(layer2)\n",
    "    optim.post_update_params()\n",
    "# print(\"### Results before backward propagation ###\")\n",
    "# Print gradients\n",
    "# print(layer1.dweights)\n",
    "# print(layer1.dbiases)\n",
    "# print(layer2.dweights)\n",
    "# print(layer2.dbiases)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(lrs, accuracies, color='blue', label='Accuracy')\n",
    "plt.xlabel('LRs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('LRs vs Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4497b629-a27d-417d-811d-e4c5280d8a02",
   "metadata": {},
   "source": [
    "### ADAM Optimizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd2c05-d1ad-439f-baf9-47be899dc9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    def __init__(self,learning_rate=0., decay=0.,momen=0.,epsilion=0.09,beta_1=0.09, beta_2=0.009):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.curr_learning_rate = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.decay = decay\n",
    "        self.momen = momen\n",
    "        self.epsilion = epsilion\n",
    "        self.iterations = 0\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.curr_learning_rate = self.learning_rate * (1.0/(1.0+self.decay*self.iterations))\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer,'weights_cache'):\n",
    "            layer.weights_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.bias)\n",
    "            layer.weights_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.bias)\n",
    "\n",
    "        layer.weights_momentums = self.beta_1*layer.weights_momentums + (1-self.beta_1)*layer.dweights\n",
    "        layer.bias_momentums = self.beta_1*layer.bias_momentums + (1-self.beta_1)*layer.dbiases\n",
    "\n",
    "        layer.weights_cache = self.beta_2*layer.weights_cache + (1-self.beta_2)*layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2*layer.bias_cache + (1-self.beta_2)*layer.dbiases**2\n",
    "\n",
    "        weight_momens = layer.weights_momentums/(1-self.beta_1**(self.iterations+1))\n",
    "        bias_momens = layer.bias_momentums/(1-self.beta_1**(self.iterations+1))\n",
    "\n",
    "\n",
    "        weight_cac = layer.weights_cache/(1-self.beta_2**(self.iterations+1))\n",
    "        bias_cac = layer.bias_cache/(1-self.beta_2**(self.iterations+1))\n",
    "\n",
    "        layer.weights += -self.curr_learning_rate* (weight_momens/(np.sqrt(weight_cac)+ self.epsilion))\n",
    "        layer.bias += -self.curr_learning_rate* (bias_momens/(np.sqrt(bias_cac)+ self.epsilion))\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20233a0-5349-4fbc-b814-9372e3f22dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(samples=100, classes=3)\n",
    "layer1 = Dense(2,64)\n",
    "activation_layer_1 = activateReLu()\n",
    "\n",
    "layer2 = Dense(64,3)\n",
    "loss_activation = Softmax_Categorical_Loss()\n",
    "optim = Optimizer_Adam(learning_rate=1.0,decay=1e-3)\n",
    "\n",
    "lrs = []\n",
    "accuracies = []\n",
    "for epoch in range (10001):\n",
    "    layer1.forward_prop(X)\n",
    "    activation_layer_1.forward_pass(layer1.output)\n",
    "    \n",
    "    layer2.forward_prop(activation_layer_1.output)\n",
    "    \n",
    "    loss = loss_activation.forward_prop(layer2.output,y)\n",
    "    \n",
    "    # print(\"### Results before forward propagation ###\")\n",
    "    # print(loss_activation.output[:5])\n",
    "    \n",
    "    \n",
    "    \n",
    "    preds = np.argmax(loss_activation.output, axis=1)\n",
    "    # print(preds)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y,axis=1)\n",
    "    # print(y)\n",
    "    \n",
    "    acc = np.mean(preds == y)\n",
    "    # print(\"Accuracy : \", acc)\n",
    "    if not epoch %100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {acc:.3f}, ' +\n",
    "              f'loss: {loss:.3f},' +\n",
    "             f'lr: {optim.curr_learning_rate}')\n",
    "        lrs.append(optim.curr_learning_rate)\n",
    "        accuracies.append(acc)\n",
    "    loss_activation.backward_prop(loss_activation.output,y)\n",
    "    layer2.backward_prop(loss_activation.dinputs)\n",
    "    activation_layer_1.backward_pass(layer2.dinputs)\n",
    "    layer1.backward_prop(activation_layer_1.dinputs)\n",
    "\n",
    "    optim.pre_update_params()\n",
    "    optim.update_params(layer1)\n",
    "    optim.update_params(layer2)\n",
    "    optim.post_update_params()\n",
    "# print(\"### Results before backward propagation ###\")\n",
    "# Print gradients\n",
    "# print(layer1.dweights)\n",
    "# print(layer1.dbiases)\n",
    "# print(layer2.dweights)\n",
    "# print(layer2.dbiases)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(lrs, accuracies, color='blue', label='Accuracy')\n",
    "plt.xlabel('LRs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('LRs vs Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19697da-9a1f-4f1d-8d72-cbb86f1e1185",
   "metadata": {},
   "source": [
    "### Testing Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a97f72-70b5-4680-b2a4-acbfc4c97981",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = spiral_data(classes=3, samples=100)\n",
    "layer1.forward_prop(X_test)\n",
    "activation_layer_1.forward_pass(layer1.output)\n",
    "layer2.forward_prop(activation_layer_1.output)\n",
    "loss = loss_activation.forward_prop(layer2.output, y_test)\n",
    "\n",
    "preds = np.argmax(loss_activation.output, axis=1)\n",
    "if (y_test.shape == 2):\n",
    "    y_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "accuracy = np.mean(preds == y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
